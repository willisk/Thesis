\chapter{Introduction}
% \label{chap:Intro}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------


Deep neural networks 
have been successfully applied to many areas 
% have shared a story of success on multiple domains
that had previously been considered difficult to solve. 
Examples include 
image classification (\cite{Image_recognition}), 
speech recognition (\cite{Speech_recognition}), 
natural language processing (\cite{gpt3}),
and reinforcement learning (\cite{AlphaZero}).
Deep neural networks have become a central part of machine learning
as a result of their versatility in application, state-of-the-art performance,
and their lack of need for designing handcrafted features.
Their success is mainly attributed to an increase in large-scale publicly available
data sets and an increase in processing power.
However, applying models that have demonstrated their success in test scenarios 
to real-world environments presents new challenges, 
as often high-quality labeled data is not readily available for specific domains.

A routine assumption in machine learning is that data 
in the training and testing environment is identically distributed. 
In reality, data distributions may differ \cite{},
leading to performance degradations for models trained with 
standard deep learning algorithms \cite{}.
Fine-tuning the model to the new domain 
% using the criterion-loss of the source task
is a straight-forward approach,
however this is prone to over-fitting when data is scarce in the target domain.

Transfer learning (\cite{XFER_SURVEY}) has emerged as a new field of machine learning
that aims 
at transferring knowledge from models, 
which have learned to perform a source task 
to a target task, by exploiting underlying commonalities.
\cite{DA_how_transferable} show that 
this leads to improved results, even in divergent tasks.
% and first layers are general/transferrable features and later layers more task-specific
Given that data set acquisition is generally a time-consuming and expensive process,
the importance of transfer learning is clear. 
Domain adaptation (DA) in particular, a subtopic of transfer learning,
addresses the problem of domain-mismatch of source and target tasks.

A domain is considered to be a pair $(\set X, \mathbb P)$ of an input space equipped with a probability distribution. DA operates under the assumption that the distributions of
the input of source and target domain have changed.
Typically, this challenge is faced by encouraging the model to learn a domain-invariant
representation of the data.
In this work, the usual approach of dealing with domain adaptation is flipped: 
the target data is transformed to fit in with the source domain.
% using well-optimized feature-representations from the model. 
An optimization objective derived from work by \cite{DeepInversion} 
is proposed and
extensively tested against numerous baselines.
A core contribution to this thesis was the creation of a comprehensive code
base\footnote{\url{https://github.com/willisk/Thesis}} for the domain adaptation experiments.  This 
allows for detailed analysis of the proposed algorithms under varying configurations 
on real-world data sets.

The results show that neural networks form highly optimized feature-representations 
that capture pertinent properties of the data set.
Information about the data set is implicitly stored in the layers of a neural network.
Also, additional information, that is stored more explicitly in batch normalization layers
can be of use.
This information can be used for domain adaption and inverse tasks, such as deconvolution.

% \cite{GithubRepo}

\subsubsection{Related work}

The biggest challenge in domain adaptation lies in measuring the difference in distributions
and in effectively bridging this discrepancy.
Most related work in domain adaptation
addresses this problem by learning domain-invariant features of the data
which classic machine learning methods can deal with.
In this effort, \cite{DA_AE} propose training an auto-encoder that is able to encode both domains.
\cite{DA_Deep_Transfer} make use of the criterion loss along with a domain confusion loss 
which is implemented by adding an adaptation layer on top of the classifier's last layer.
This auxiliary layer aims to make the learned representations indistinguishable 
between domains by maximally confusing a domain classifier.
\cite{DA_MMD} further extend the use of adaptation layers to multiple layers throughout the network.
\cite{Deep_Coral} formulate a distribution loss by using second-moment statistics, 
the covariances of feature activations.
\cite{DA_CMD} propose the Central Moment Discrepancy incorporating any number of higher-order moments.
% Proves convergence for distributions of single features.
\cite{DA_Prototypes} propose to classify data of the target domain by comparing 
feature-representations to prototypes of each class.
\cite{DA_CycleGAN} propose CycleGAN, a generative adversarial network that learns 
explicit domain transformations, by formulating an adversarial loss
that measures the reconstruction accuracy.

Set in the context of inverse problems, 
\cite{IP_Deconv} and \cite{IP_VN} use convolutional neural networks 
and residual networks for image deconvolution by minimizing a reconstruction loss
after an artificial blur kernel is applied to training data.



\subsubsection{Classification}

\cite{DA_SURVEY} distinguish the homogeneous setting 
where $\set X_s$ and $\set X_t$, the source and target input space, are the same,
but the corresponding distributions differ.
They further classify work in this area by their loss formulation.
This work explores the use of both, a class-criterion
and a statistics-criterion.
The class-criterion
uses class labels available in the target domain
and the statistics-criterion (\cite{DA_SURVEY}) 
minimizes the mismatch in distributions.

\subsubsection{Contribution}

This work addresses domain adaptation by learning an explicit function 
that maps the 
target domain to the source domain via residual networks.
This is in contrast to previously mentioned work, 
where the classifying neural network itself is modified 
in order to make features domain-invariant.
Leaving the original model intact without modifications is a desirable property 
for security and robustness certification (\cite{NEURIPS2019_f7fa6aca}).
Furthermore, the cited work on domain adaptation assumes access to the source data set.
The proposed methods do not require sharing the full source data set;
instead, the data statistics measured in an appropriate feature-space are sufficient.
These statistics are readily available in networks that make use of
the widely adapted batch normalization layers.
Also, an explicit domain mapping allows for
comparing the result to a ground truth.
This permits additional evaluation metrics along with the possibility of visual inspection,
greatly simplifying a final assessment of success.
Additionally, the data-privacy aspect of sharing these statistics
is considered.

This work also contributes to the domain of inverse problems,
more specifically, to the task of deblurring or deconvolution 
for image reconstruction.
Whereas \cite{IP_Deconv} and \cite{IP_VN}, 
% such as \cite{IP_Deconv} and \cite{IP_VN} 
train a deconvolution model for
reconstruction on handcrafted perturbation functions, 
this work makes use of the prior knowledge
learned by an image classification network 
to correct for general distribution shifts
that also entail deconvolutions.

This work examines the unsupervised and semi-supervised settings.

% The class-dependent statistics loss introduced in \cref{chap:Background} 
% can be seen as expanding on the idea of using multiple prototypes.

An example of a practical application is
the degradation of a sensor over time.
Here, it can provide a method for adaptation or re-calibration.


\subsubsection{Outline}

\Cref{chap:Background} clears some mathematical preliminaries and establishes the notation 
used throughout. Neural networks and, 
more specifically, convolutional neural networks are briefly introduced.
\Cref{chap:Objective} outlines the main objectives of this work and 
states the problem formally.
\Cref{chap:Experiments} explains in detail the experimental setup.
It introduces the various methods and the reconstruction and perturbation models.
Further, the data sets and the evaluation procedure are presented.
In \cref{chap:Results} the final results and conclusions are given.

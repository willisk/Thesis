\chapter{Introduction}
% \label{chap:Intro}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------


Deep neural networks have been successfully applied to
many areas that had previously been considered difficult to solve. 
Examples include 
image classification (\cite{Image_recognition}), 
speech recognition (\cite{Speech_recognition}), 
natural language processing (\cite{gpt3}),
and reinforcement learning (\cite{AlphaZero}).
They have become a central part of machine-learning
as a result of their versatility in application, state-of-the-art performance,
and their lack of need for designing handcrafted features.
Their success is largely attributed to an increase in large-scale publicly available
data sets and an increase in computing power.
However, applying models that have demonstrated their success in test scenarios 
to real-world environments presents new challenges, 
as often high-quality labeled data is not readily available for specific domains.

A routine assumption in machine-learning is that data 
in the training and testing environment are identically distributed. 
In reality, data distributions may differ \cite{},
leading to performance degradations for models trained with 
standard deep learning algorithms \cite{}.
Fine-tuning the model to the new domain 
% using the criterion-loss of the source task
is a straight-forward approach,
however this is prone to over-fitting when data is scarce in the target domain.

Transfer-learning (\cite{XFER_SURVEY}) has recently emerged as a new field that is focused on 
transferring knowledge from machine-learning models which have learned to perform a source task 
to a target task by exploiting underlying commonalities.
\cite{DA_how_transferable} has shown that 
this leads to improved results, even in divergent tasks.
% and first layers are general/transferrable features and later layers more task-specific
Given that the process of data set acquisition is generally a time-consuming and expensive process,
the importance of transfer-learning is clear. 
Domain-adaptation (DA) in particular, a subtopic of transfer-learning,
addresses the problem of a domain-mismatch for a source and target task.

A domain is considered to be a pair $(\set X, P)$ of an input space equipped with a probability distribution. DA operates under the assumption that the distributions over the input of source and target domain have changed.
Typically, this challenge is faced by encouraging the model to learn a domain-invariant
representation of the data.
In this work, the usual script on dealing with domain-adaptation is flipped: 
the target data is transformed using well-optimized feature-representations from the model. 
An optimization objective inspired by \cite{DeepInversion} is proposed and
extensively tested against numerous baselines.
As a core contribution to this thesis, a comprehensive code
base\footnote{\url{https://github.com/willisk/Thesis}} for the domain-adaptation experiments 
was built, 
allowing for detailed analysis of the proposed algorithms under varying configurations 
on real-world as well as toy data sets.

The results show that...

% \cite{GithubRepo}

\subsubsection{Related work}

The biggest challenge in domain-adaptation lies in measuring the difference in distributions
and in effectively bridging this discrepancy.
Most related work domain-adaptation 
addresses this problem by learning domain-invariant feature-representations of the data
which classic machine-learning methods can deal with.
In this effort, \cite{DA_AE} propose training an auto-encoder that is able to encode both domains.
\cite{DA_Deep_Transfer} make use of the criterion loss along with a domain confusion loss 
which is implemented by adding an adaptation layer on top of the classifier's last layer.
This auxiliary layer aims to make the learned representations indistinguishable 
between domains by maximally confusing a domain classifier.
\cite{DA_MMD} further extend the use of adaptation layers to multiple layers throughout the network.
\cite{Deep_Coral} formulate a distribution loss by using second-moment statistics, the covariances of features and
\cite{DA_CMD} propose the Central Moment Discrepancy incorporating any number of higher-order moments.
% Proves convergence for distributions of single features.
\cite{DA_Prototypes} propose to classify data of the target domain by comparing 
the feature-representations to a prototype of each class.
\cite{DA_CycleGAN} propose CycleGAN, a generative adversarial network that learns 
explicit domain transformations, by proposing an adversarial loss
that measures the reconstruction accuracy.

Set in the context of inverse problems, 
\cite{IP_Deconv} and \cite{IP_VN} use convolutional neural networks 
and residual networks for image deconvolution by minimizing a reconstruction loss
after an artificial blur kernel is applied to training data.



\subsubsection{Classification}

\cite{DA_SURVEY} distinguish the homogeneous setting 
where $\set X_s$ and $\set X_t$, the source and target input space, are the same,
but the individual distributions are have changed.
They further classify related work by their loss formulation.
This work explores the use of both, a class-criterion,
which uses class labels available in the new domain, 
and a statistics-criterion (\cite{DA_SURVEY}) that minimizes 
the mismatch in distributions.
The and the labeled setting.

\subsubsection{Contribution}

This work addresses domain-adaptation by learning an explicit function 
that maps the 
target domain to the source domain via residual networks.
This is in contrast to previously mentioned work, 
where the classifying neural network itself is modified 
in order to make features domain-invariant.
Leaving the original model intact without modifications is a desirable property 
for safety and robustness certification (\cite{NEURIPS2019_f7fa6aca}).
Previously mentioned work assumes access to the source data set.
The proposed methods do not require sharing the full source data set;
the data statistics measured in an appropriate feature-space are sufficient.
These statistics are readily available in networks that make use of
the widely adapted batch-normalization layers.
Also, an explicit domain mapping function allows for
comparing the result to a ground truth.
This permits additional evaluation metrics along with the possibility of visual inspection,
greatly simplifying a final assessment of success.
Furthermore the data-privacy aspect of sharing these statistics
is considered as well.

This work can also be seen as contributing to the domain of inverse problems.
More specifically, the task of deblurring, deconvolution or image reconstruction.
Whereas the previously mentioned work in this area, 
% such as \cite{IP_Deconv} and \cite{IP_VN} 
trains a deconvolution model for
reconstruction of handcrafted perturbation functions, 
this work makes use of the prior knowledge
learned by an image classification network 
to correct for general distribution shifts, which also applies to deconvolution.

This work examines both, the unsupervised and supervised settings.

% The class-dependent statistics loss introduced in \cref{chap:Background} 
% can be seen as expanding on the idea of using multiple prototypes.

An example use-case is given for distribution-shifts that occur over time,
possibly due to a changing environment or degrading sensors.
Here, it can provide a method for adaptation or re-calibration.

This work was originally inspired by \cite{DeepInversion} 
which was aimed
at model distillation.


\subsubsection{Outline}

\Cref{chap:Background} clears some mathematical preliminaries and establishes the notation 
used throughout. Neural networks and, 
more specifically, convolutional neural networks are briefly introduced.
\Cref{chap:Objective} outlines the main objectives of this work and 
states the problem formally.
\Cref{chap:Experiments} explains in detail the experimental setup.
It introduces the various methods and the reconstruction and perturbation models.
Further, the data sets and the evaluation procedure is presented.
\Cref{chap:Results} presents the final results and conclusions.

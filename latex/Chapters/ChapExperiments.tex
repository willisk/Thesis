
\chapter{Experiments}
\label{chap:Experiments} 

\section{Methods}

The success of the reconstruction largely depends on the representation of the data
after the feature-map is applied.
Although, assuming that the data distribution of the data can be accurately described
by a mean and a variance is a strong assumption.
More sophisticated ways of modeling data distributions exist
\XXX{insert some}
, yet mean and variance were chosen due to their simplicity in evaluation and computing of the gradients.

The feature-map will ideally "disentangle" the data space to enable 
simple statistics to capture the complexity of the data set.

It is hypothesized that neural networks "disentangle" the input space in order to end up with an efficient feature-representation in its later layers.
(This becomes more apparent when one imagines that most neural networks must learn to classify by linearly separating its features obtained from the previous to last layer.)


For assessing the efficacy of the ability to capture the data distribution by
the statistics of the intermediary representations in a neural network,
several different mappings or collections of mappings will be compared. 

For every method there exists a class-dependent variant.
\subsection{neural network}

A \textbf{neural network} $\Phi$ is a function that maps an input $\vec x \in \R^d$ to a label $y$.
Normally, in classification tasks, where one hopes to assign a label to an input, the output of a neural network is a C-dimensional vector..
\XXX{maybe not, logit; DEF}. 
A \textbf{multi-layer perceptron} (MLP) is a special type of neural network. Throughout this work, when referring to a neural network, a multi-layer perceptron is meant. A MLP is a composition of functions $\Phi_i$, also called \textbf{layers} that successively act on the outputs $\vec h$ of the previous layer, called \textbf{activations} or \textbf{hidden states}. 
The last layer is referred to as the \textbf{logits-layer}. 
The predicted output label $\hat y$ is normally obtained by returning the index of the maximum value in the logits-layer, thus $\R^{\textnormal L} = \R^C$.
In a plain \textbf{fully-connected} neural network, the layers are each made up of an affine linear transformation and a non-linear activation function.

Formally, a fully-connected neural network of layer depth L is described as:

\[
    \Phi = \Phi_\text{L} \circ \ldots \circ \Phi_1
\]
\[
    \vec h_\ell = (\Phi_\ell \circ \ldots \circ \Phi_1) (\vec x) =
    \Phi_\ell(\vec h_{\ell-1}) \in \R^{d_\ell}
\]
$\Phi_\ell$ then, is a mapping from $\R^{d_{\ell-1}}$ to $\R^{d_\ell}$. 
The space of a hidden state $\vec h$ is called a \textbf{feature-space}.
The input $\vec x$ is also referred to as $\vec h_0$ and $\R^{d_0} := \R^d$.

\[
    \Phi_\ell(\vec h) = \sigma (\vec W \vec h + \vec b) \comma{where}
    \vec W \in \R^{d_\ell \times d_{\ell-1}} \comma{and}
    \vec b \in \R^{d_\ell}
\]
The affine linear transformation is made up of a \textbf{weight matrix} $\vec W$ and a \textbf{bias} $\vec b$.
The non-linear activation function $\sigma$ is often one of \textit{ReLU} (Rectified Linear Unit), \textit{tanh} or \textit{softmax}. The former two are mappings from $\R$ to $\R$ that are applied element-wise.




\subsection{neural network - last layer}

Given a neural network $\Phi$ of depth L..
A feature-mapping can be obtained by mapping inputs to the second-to-last layer.
Generally, this is seen as a layer with the best feature representation? source?


\[
    \varphi : \R^d \to \R^{d_\textnormal{L-1}} = (\Phi_\textnormal{L-1} \circ \dots \circ \Phi_1)
\]
\subsection{neural network - all layers}
All hidden states of the neural network are used to produce a collection of mappings [$\varphi_0, \dots, \varphi_\text{L}$].
\begin{alignat*}{2}
    \varphi_\ell &: \R^d \to \R^{d_\ell} &&= (\Phi_\ell \circ \dots \Phi_1) \quad
    \text{for $\ell = 1, \dots$, L - 1} \\
    \varphi_\textnormal{L} &: \R^d \to \R^d &&= \Id
\end{alignat*}





\subsection{random projections}
The method of projecting the input to the hidden representations of a neural network will be contrasted with taking $n$ random linear projections.
A random projection is a linear mapping $r: \R^d \to \R$
It is created by choosing a normalized random vector $\vec v \in S^{d-1} = \{\vec x \in \R^d : \|\vec x\| = 1\}$.
It is a simple linear projection on to the one-dimensional subspace defined by the vector $\vec v$.
\[
    r(\vec x) = \vec v ^\top \vec x
\]
By choosing a total of n random vectors $\{v_1, \dots, v_n\}$ one obtains a linear mapping $R: \R^d \to \R^n$:
\[
    R(\vec x) = \vec V \vec x =
    \begin{bmatrix}
        - \vec v_1 ^\top - \\
        \vdots \\
        - \vec v_n ^\top - \\
    \end{bmatrix}
    \vec x =
    \begin{bmatrix}
        \vec v_1 ^\top \vec x \\
        \vdots \\
        \vec v_n ^\top \vec x \\
    \end{bmatrix}
\]
%
The loss function $\loss_R$ stays as was defined before.
Though it might be more reasonable to have the projection centered around a more suitable position than to just project from the origin, in order to obtain better scaled values.
% 
\[
    R_{\vec o} (\vec x) = \vec V (\vec x - \vec o) \,,
\]
where $\vec o \in \R^d$ is the new origin of the projection. Although, practically one often works with normalized data anyhow..
The class-dependent variant can be modified to select an origin $\vec o_c$ for each class $c$.
% 
\begin{align*}
    \loss _R ^{\mathcal C} (\set A, \set B) &=
    \begin{aligned}[t]
        \sum _{\substack{c = 1 \dots C \\ \set A|_c ,\, \set B|_c \neq \emptyset}} 
        \|\mean ({R_{\vec o_c} (\set A|_c)}) - \mean ({R_{\vec o_c} (\set B|_c)}) \| \\
        {} + \|\var ({R_{\vec o_c} (\set A|_c)}) - \var ({R_{\vec o_c} (\set B|_c)}) \| 
    \end{aligned}
\end{align*}

The idea is to set the origin to be at the center of each class of the target data set $\set A$. $\vec o_c = \mean ({\set A|_c})$
This way we can ensure a balanced output..



\subsection{random projections ReLU}
To further explore the influence of the non-linear activation functions contained within the network,
one can combine the previous method with adding an activation function, in this case the ReLU.
% 
\[
    R_{\vec o}^+ (\vec x) = (\vec V (\vec x - \vec o))^+ \,,
\]
where $(\,\cdot\,)^+ :\R^n \to \R^n$ is the projection onto the positive orthant. It applies $\max(0, \cdot)$ element-wise.

Since ReLUs have a bias parameter that shifts the threshold where an input can pass, this will also be incorporated.
This bias parameter usually doesn't come from the ReLU itself, but is incorporated in the previous layer's affine transformation.
\[
    R_{\vec o}^{\vec b} (\vec x) = (\vec V (\vec x - \vec o) + \vec b)^+ \,,
\]
where $\vec b \in \R^d$ is the bias. For a target dataset $\set A$, it is chosen as $\vec b \sim \mathcal N(\boldsymbol \mu, \textnormal{diag}(\boldsymbol \sigma ^2))$, 
where $\boldsymbol \mu = \mean ({R_{\vec o}(\set A)})$ and $\boldsymbol \sigma ^2 = \var ({R_{\vec o}(\set A)})$.

The class-dependent variant can again make use for more suited origins of projection $\vec o_c$ and individual biases $\vec b_c$.
$\vec b_c$ is chosen at random to be centered around the output $\sim \mathcal N(\boldsymbol \mu_c, \textnormal{diag}(\boldsymbol \sigma _c^2))$, 
where $\boldsymbol \mu_c = \mean ({R_{\vec o}(\set A|_c)})$ and $\boldsymbol \sigma_c ^2 = \var ({R_{\vec o}(\set A|_c)})$ for all non-empty $\set A|_c$.
% 
\begin{align*}
    \loss _{R^+} ^{\mathcal C} (\set A, \set B) &=
    \begin{aligned}[t]
        \sum _{\substack{c = 1 \dots C \\ \set A|_c ,\, \set B|_c \neq \emptyset}} 
        \|\mean ({R_{\vec o_c}^{\vec b_c} (\set A|_c)}) - \mean ({R_{\vec o_c}^{\vec b_c} (\set B|_c)})\| \\
        {} + \|\var ({R_{\vec o_c}^{\vec b_c} (\set A|_c)}) - \var ({R_{\vec o_c}^{\vec b_c} (\set B|_c)}) \| 
    \end{aligned}
\end{align*}

\subsection{randomly initialized neural network}
To study the importance of an optimized feature-representation of a trained neural network, 
the same neural network model with randomly initialized parameters will be evaluated and compared.

\subsection{combinations}
Combinations of all previously defined losses and feature-maps can be made. In particular, the combination of all neural network layers and random projections will be examined in order to report any improvement.


\section{Data Sets}
\label{sec:datasets}

\subsection{GMM}
\label{sec:datasetgmm}
A dataset made up of Gaussian mixture models (in the following \textit{GMM}), 
is made up of $C$ classes, each of which contains $n_\text{mode}$ clusters or modes of multivariate Gaussian normal distributions. The probability-density function is as follows.
\begin{align}
\label{eqn:gmmdistr}
    p(\, \vec x \mid c \,) = \frac 1 {n_\text{mode}} \sum _{m=0}^{n_\text{mode}}
    \mathcal N (\gamma \vec m_c + \lambda \boldsymbol \mu_c^{(m)}, \boldsymbol \Sigma_c^{(m)}) \, ,
\end{align}
where \\
% $\boldsymbole \theta = (
\XXX{upper indices for c?}
$\vec m_c \sim \mathcal N (\vec 0, \vec I)$ is the center for each class $c=1,\ldots, C$ ,\\
$\boldsymbol \mu_c^{(m)} \sim \mathcal N (\vec 0, \vec I)$ and
$\vec \Sigma_c^{(m)}$ are the center and the covariance matrix of the multivariate normal distribution
for each mode $m=1,\ldots, n_{\textnormal{mode}}$ and class $c=1,\ldots, C$.  \\
A positive semi-definite matrix $\vec \Sigma_c^{(m)}$ is generated by choosing $d$ eigenvalues $\vec e = (e_1, \ldots, e_d)$, $e_i \sim \mathcal U(\alpha, \beta)$, for all $i=1, \ldots, d$, for some $\alpha, \beta > 0$ and 
by sampling a random orthogonal matrix $\vec Q$. 
This can be done for example by choosing $\vec Q = \{q_{ij}\}_{i,j=0}^{d}$, 
where $q_{ij} \sim \mathcal N(0, 1)$
and creating an orthonormal basis via Gram-Schmidt. 
\begin{align*}
    \vec \Sigma_c^{(m)} = \vec Q^\top \text{diag}(\vec e) \vec Q
\end{align*}

\XXX{plot example}
For a given data set size, the labels are generated by equally sampling from all classes $1, \ldots, C$.
Then the input will be sampled according to the conditional probability-density function given by \eqnref{eqn:gmmdistr}.
The specific parameters used for generating the data set in all the experiments
are given in the appendix \nameref{AppendixA} in table \ref{tab:GMMParams}.



\subsection{MNIST}
MNIST is a dataset of xxx black-and-white images of handwritten digits from 0 to 9. 
Each Image is made up of 28x28 pixels, total 784.

\subsection{CIFAR-10}
CIFAR10 contains xxx colored images from 10 non-overlapping categories.
Each Image has 3 Channels and 32x32 pixels, totaling a dimension of 3072.





\section{Perturbation and Reconstruction Models}


\subsection{Gaussian Mixture Models}

The \textbf{perturbation model} for the GMM data set is
a random affine-linear transformation, controlled by a parameter $\lambda \in \R^+$.
The linear part is the identity transformation with added Gaussian noise
and the translation vector is also sampled with Gaussian noise.
The noise is multiplied by $\lambda$.
\[
    \delta(\vec x) = (\vec I + \lambda \vec N)\vec x + \lambda \mu \,,
\]
where $\vec N = \{n_{i, j}\}_{i j = 1}^{d}$, $n_{ij} \sim \mathcal N (0, 1)$ for all $i, j = 1 , \ldots, d$
and 
$\mu = \{\mu_i\}_{i=0}^d$, $\mu_i \sim \mathcal N(0, 1)$ for all $i=1,\ldots,d$.
This transformation is in invertible if $\det (\vec I + \lambda \vec N) \neq 0$ with P=1,
\XXX{talk about invertibility?}
For $\lambda = 0$ the identity transformation is obtained.

The \textbf{reconstruction model} is also an affine-linear transformation. 
It is initialized with the identity transformation.

If $\rho(\vec x) = \vec A \vec x + \vec b$ is the reconstruction model, then an optimal solution is given by
$\vec A = (\vec I + \lambda \vec N)^{-1}$, $\vec b = -\lambda \vec A \boldsymbol \mu$:
\begin{align*}
    \delta ( \rho (\vec x)) &= (\vec I + \lambda \vec N)(\vec A \vec x + \vec b) + \lambda \boldsymbol \mu \\
    &= (\vec I + \lambda \vec N)\vec A (\vec x  - \lambda \boldsymbol \mu) + \lambda \boldsymbol \mu \\
    &= \vec x  -\lambda \boldsymbol \mu + \lambda \boldsymbol \mu \\
    &= \vec x \\
\end{align*}

\subsection{Image Data Sets}

\subsubsection{Perturbation model}

The \textbf{perturbation model} for the image data sets MNIST and CIFAR10
is a composition of additive Gaussian noise and two noise-controlled 2d-convolutions.
\XXX{talk about 2d-convolutions?}

\[
    \delta(\vec x) = \vec K_2(\vec K_1(\vec x + \mu)
\]

The convolutions $\vec K_n$ are given by kernel matrices $\vec k_n$ of size $3\times3$ for $n=1,2$.
The kernel matrices are set to the identity kernel with added Gaussian noise, controlled by $\lambda$.
The identity kernel for the MNIST data set (which only contains one "color" channel) is given by 
% \eqnref{eqn:kernelid}.
\begin{equation*}
% \label{eqn:kernelid}
    \vec k_{\text{id}} = \begin{bmatrix}
        0 & 0 & 0 \\
        0 & 1 & 0 \\
        0 & 0 & 0 \\
    \end{bmatrix} \,.
\end{equation*}
For the CIFAR10 data set, where 3 color channels are present, the input shape is of (3, 32, 32). 
In order to obtain the same output shape, 
a total of $3*3=9$ convolution kernels $\vec k^{[i,j]}$ for $i,j=1,2,3$ are needed per convolution.
The output of channel $j$ is given by $\sum_{i=1}^{3} \vec x * \vec k^{[i, j]}$.
The identity convolution can be achieved by setting 
\[
    \vec k^{[i,j]} = \begin{cases}
        \vec k_{\text{id}} &, \text{ if } i = j\\
        \vec 0 &, \text{ otherwise} \\
    \end{cases} \,.
\]

This convolution operator is in general not invertible for $\lambda > 0$.
\XXX{Proof??}

For the output to contain the same dimensions as the input, the input needs to be \textit{padded}.
The 'reflection' padding was chosen.
\XXX{Should elaborate}


\subsubsection{Reconstruction model}

\begin{figure}
\begin{minipage}{0.5\textwidth}
\centering
\input{Figures/ChartInvertblock}
% \caption*{ResNet - Residual Layer}
\end{minipage}
\begin{minipage}{0.5\textwidth}
\centering
\input{Figures/ChartInvertNet}
% \caption*{ResNet - Residual Layer}
\end{minipage}
\caption{ResNet architecture}
\label{fig:resnet}
\XXX{add ReLU out + 1x1conv}
\end{figure}

The model for the reconstruction task on images is given by a residual network architecture
as outlined in \ref{fig:resnet}. 
The network \textit{width} - the number of individual convolutional kernels - is controlled by the parameter $s_\text{width}$, the \textit{depth} - the number of residual layers - by the parameter
$s_\text{depth}$.

The default parameters for the experiments are given in \ref{tab:CIFARParams}.




\subsection{Evaluation}

\input{Figures/ChartEvaluation}


For the main reconstruction task, 
five metrics will be studied to determine a methods success (along with visual appeal).
For one, the accuracy of the reconstructed data set will be measured by the neural network.
The relative l2-error, the peak signal-to-noise ratio, the accuracy of the neural network and the accuracy on a verifier network.
Alongside, a validation data set $\set C$ will measure generalization of the found reconstruction $\rho$ to a data set to which it was not optimized for.
Since the original neural network was also used in the optimization process, a verifier network $\Phi_{\text{ver}}$ will be used as a second evaluation of the accuracy.

If the perturbation $\delta$ is known, then one can calculate the \textbf{relative error} of the identity vectors. 
\begin{equation}
\label{eqn:relerror}
    \varepsilon_F = \frac {\|\widehat {\vec I_d} - \vec I_d\|_F} {\|\vec I_d\|_F} \,,
\end{equation}
where $\widehat {\vec I_d} = \begin{pmatrix} \rho (\delta (\vec e_1), \dots, \rho (\delta (\vec e_d) \end{pmatrix}$ and $\vec e_i$ is the i-th unit vector.

Then \eqnref{eqn:relerror} becomes
\[
    \varepsilon_F = \frac 1 {\sqrt d} \sqrt{ \sum_{i=0}^d \|\rho (\delta (\vec e_i)) - \vec e_i)\|_2^2}
\]

For the GMM data set, where the distortion and reconstruction models are affine-linear transformations,
this becomes..
\[
    \varepsilon_F = (\vec I + \lambda \vec N) (\vec A \vec x + \vec s) + \lambda \mu  - e_i 
\]
\[
    \rho ( \delta ( \vec x)) = (\vec I + \lambda \vec N) \vec A \vec x +(\vec I + \lambda \vec N) \vec s + \lambda \mu 
\]

The \textbf{PSNR} (peak signal-to-noise ratio) between $\vec x$ and $\vec y$ is calculated as follows.
\[
    PSNR(\vec x, \hat {\vec x}) = 20 \log_{10} \left (\frac {\max_i(\vec x_i)} {\|\vec x-\hat {\vec x}\|_2} \right )
\]
It is a common measure of image quality when assessing image compression algorithms and is measured in decibels db.
It is similar to the relative error in that it uses the mean-squared error, though it assessed on the images directly.
For a batch of images, the score is averaged over the images to give a mean PSNR score.

\subsection{Results}

\chapter{Implementation Details}
\label{AppendixGlossary}


The implementation of this work was done in Python\footnote{\url{https://www.python.org/}}
using the popular machine-learning library PyTorch\footnote{\url{https://pytorch.org/}}.
A copy of the code repository along with all the results 
is made publicly available on Github\footnote{\url{https://github.com/willisk/Thesis}}.

\begin{table}[!h]
\caption*{\Large\textbf{Glossary}\vspace{1cm}}
\pgfplotstabletypeset[
glossary,
every row no 5/.style={extra spacing},
]{
scalar & $s$ & small letters are used for scalars
vector & $\vec v$ & bold-face small letters are used for vectors,
&& the elements are denoted as $v_i$
matrix & $\vec M$ & bold-face capital letters are used for matrices,
&& the elements are denoted as $M_{i,j}$
identity matrix & $\vec I$ & denotes the identity matrix
%
sample input & $\vec x$ & element of input space $\set X$
input space & $\set X$ & vector space $\R^d$
data set dimension & $d$ & dimension of vector space $\set X$
input shape & $(d_1, d_2,\ldots)$ & shape of input sample $\vec x \in \R^d=\R^{d_1\times d_2 \times \ldots}$
sample label & $y$ & label $y \in \set Y$
label set &$\set Y$ & discreet set $\set Y = \{1,\ldots, C\}$
number of classes &$C$ & number of distinct labels/classes of the data set
sample & $\vec z$ & sample of $\set Z$; tuple of input and label $\vec z = (\vec x, y)$
sample space & $\set Z$ & $\set Z = \set X \times \set Y$
%
& $\finiteset{\, \cdot \,}$ & denotes the set of all finite non-empty subsets
target data set & $\set A$ & data set $\set A = \{\vec z^{(i)}\}_{i=1}^{|\set A|}$ containing samples of $\set Z$;
&&used for training the main network, 
&&and for target data set statistics
source data set & $\set B$ & used in optimization,
&&where source data set statistics are to 
&&match target data set statistics
validation data set & $\set C$ & used for evaluating validation accuracy
class-constrained batch & $\set A|_c$ & subset of $\set A$ only containing samples of label c 
%(as defined in \ref{eqn:class_constrained})
input mean & $\mean$ & empirical data set sample input mean (defined in \ref{eqn:sample_mean})
input variance & $\var$ & empirical data set sample input variance (defined in \ref{eqn:sample_var})
feature-mapping & $\varphi(\set A)$ & maps a data set to feature-space (defined in \ref{eqn:feature_mapping})
statistics-loss & $\loss_\varphi$ & loss/objective function for given feature-mapping $\varphi$ 
&&(defined in \ref{eqn:statistics_loss})
class-loss & $\lossCC_\varphi$ & class-dependent formulation of loss function (defined in \ref{eqn:class_statistics_loss})
normal distribution & $\mathcal N (\boldsymbol \mu, \boldsymbol \Sigma)$ & multivariate normal distribution 
&&with mean $\boldsymbol \mu$ and covariance-matrix $\boldsymbol \Sigma$
uniform distribution & $\mathcal U (a, b)$ & continuous uniform distribution over interval $(a, b)$
}
\end{table}
